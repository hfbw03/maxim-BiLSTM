{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1nja3002K6d"
   },
   "source": [
    "# **Sentiment Analysis of Google Play Store Reviews Using BiLSTM and fastText Word Embedding**\n",
    "Model *deep learning* yang dibangun menggunakan *PyTorch* dan *TorchText* untuk mengklasifikasikan sentimen ulasan aplikasi menggunakan data yang diambil dari *Google Play Store*.\n",
    "\n",
    "(Kode ini untuk skripsi)\n",
    "\n",
    "1. [Pengumpulan Data](#1-pengumpulan-data)\n",
    "2. [Preprocessing Data](#2-preprocessing-data)\n",
    "    - Text Cleaning\n",
    "    - Case Folding\n",
    "    - Tokenization\n",
    "    - Stopword Removal\n",
    "    - Stemming\n",
    "3. [Pelabelan Data](#3-pelabelan-data)\n",
    "4. [Pembagian Data](#4-pembagian-data)\n",
    "5. [Ekstraksi Fitur fastText](#5-ekstraksi-fitur-fasttext)\n",
    "6. [Pembentukan dan Pelatihan Model BiLSTM](#6-pembentukan-dan-pelatihan-model-bilstm)\n",
    "7. [Klasifikasi Sentimen](#7-klasifikasi-sentimen)\n",
    "8. [Evaluasi Model](#8-evaluasi-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# **1. Pengumpulan Data**\n",
    "Pada tahap pengumpulan data, penelitian ini melakukan scraping data ulasan pada Google Play Store terhadap aplikasi Maxim dengan menggunakan package google-play-scraper. Data yang diambil berjumlah 10.000 ulasan berbahasa Indonesia terbaru pada aplikasi Maxim.\n",
    "\n",
    "Dalam ulasan aplikasi, sentimen positif sering kali lebih dominan daripada negatif. Jika kita mengambil data secara acak, kemungkinan besar kita mendapatkan bias ke kelas positif, yang dapat menyebabkan model menjadi terlalu condong ke sentimen positif. Mengambil jumlah yang sama untuk ulasan positif dan negatif (rating 1-2 untuk negatif, 4-5 untuk positif) membantu menghindari bias ini, sehingga model dapat belajar secara merata dari kedua sentimen, tanpa adanya ketidakseimbangan yang terlalu ekstrim diantara kedua kelas data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package yang diperlukan untuk analisis sentimen\n",
    "from google_play_scraper import app, Sort, reviews \n",
    "import os                               \n",
    "import json                                         \n",
    "import csv                                          \n",
    "import pandas as pd                                 \n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from itertools import product\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torchtext.legacy\n",
    "from torchtext.legacy import data \n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import gensim\n",
    "import compress_fasttext\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "import ast\n",
    "from googletrans import Translator\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah working directory\n",
    "os.chdir('D:\\Dokumen\\Skripsi\\Referensi\\Kode\\SA_BiLSTM+fastText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kode scraping data ulasan aplikasi Maxim\n",
    "# Fungsi untuk memberikan label berdasarkan rating ulasan\n",
    "def label_sentiment(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 'negatif'\n",
    "    elif rating in [4, 5]:\n",
    "        return 'positif'\n",
    "    return None  \n",
    "\n",
    "# Variabel untuk menyimpan ulasan berlabel positif dan negatif\n",
    "low_rating = []     \n",
    "high_rating = []   \n",
    "target_count = 2500\n",
    "\n",
    "# Proses scraping ulasan\n",
    "while len(low_rating) < target_count or len(high_rating) < target_count:\n",
    "    reviews_data, continuation_token = reviews(\n",
    "        'com.taxsee.taxsee',    \n",
    "        lang='id',   \n",
    "        country='id',\n",
    "        sort=Sort.NEWEST,    \n",
    "        count=100000            \n",
    "    )\n",
    "\n",
    "    # Proses setiap ulasan\n",
    "    for review in reviews_data:\n",
    "        if len(low_rating) >= target_count and len(high_rating) >= target_count:\n",
    "            break\n",
    "\n",
    "        rating = review.get('score')\n",
    "        sentiment = label_sentiment(rating)\n",
    "\n",
    "        if sentiment == 'negatif' and len(low_rating) < target_count:\n",
    "            low_rating.append(review)\n",
    "        elif sentiment == 'positif' and len(high_rating) < target_count:\n",
    "            high_rating.append(review)\n",
    "\n",
    "    if len(low_rating) >= target_count and len(high_rating) >= target_count:\n",
    "        break\n",
    "\n",
    "    if continuation_token is None:\n",
    "        break\n",
    "\n",
    "# Gabungkan ulasan dengan rating rendah dan tinggi\n",
    "combined_reviews = low_rating + high_rating\n",
    "\n",
    "# Simpan hasil scraping ke dalam file CSV\n",
    "with open('combined_reviews.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "    fieldnames = ['content', 'score'] \n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for review in combined_reviews:\n",
    "        writer.writerow({\n",
    "            'content': review.get('content', ''),  \n",
    "            'score': review.get('score', '')      \n",
    "        })\n",
    "\n",
    "print(\"Data ulasan berhasil disimpan ke 'combined_reviews.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kode scraping data ulasan aplikasi Maxim (versi padat)\n",
    "def label_sentiment(rating):\n",
    "    return 'negatif' if rating in [1, 2] else 'positif' if rating in [4, 5] else None\n",
    "\n",
    "# Target jumlah ulasan untuk masing-masing kategori\n",
    "target_count = 5000\n",
    "low_rating, high_rating = [], []\n",
    "\n",
    "# Proses scraping ulasan\n",
    "while len(low_rating) < target_count or len(high_rating) < target_count:\n",
    "    reviews_data, continuation_token = reviews(\n",
    "        'com.taxsee.taxsee', lang='id', country='id', sort=Sort.NEWEST, count=100000\n",
    "    )\n",
    "    for review in reviews_data:\n",
    "        sentiment = label_sentiment(review.get('score'))\n",
    "        if sentiment == 'negatif' and len(low_rating) < target_count:\n",
    "            low_rating.append(review)\n",
    "        elif sentiment == 'positif' and len(high_rating) < target_count:\n",
    "            high_rating.append(review)\n",
    "        if len(low_rating) >= target_count and len(high_rating) >= target_count:\n",
    "            break\n",
    "    if continuation_token is None:\n",
    "        break\n",
    "\n",
    "# Gabungkan ulasan dengan rating rendah dan tinggi    \n",
    "combined_reviews = low_rating + high_rating\n",
    "\n",
    "# Simpan hasil scraping ke dalam file CSV\n",
    "with open('combined_reviews.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "    fieldnames = ['content', 'score']  \n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for review in combined_reviews:\n",
    "        writer.writerow({\n",
    "            'content': review.get('content', ''),  \n",
    "            'score': review.get('score', '')      \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "nFyis9DPYpmg",
    "outputId": "2c58e9c9-034e-48dd-9089-5b65d7950ece"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1bec4fb8-e56a-457d-894f-fae7f3b6e53b",
       "rows": [
        [
         "0",
         "content",
         "score"
        ],
        [
         "1",
         "maksa",
         "1"
        ],
        [
         "2",
         "tolong perbaiki mapsnya.maps ojol ko.kaya maps gtaðŸ¤£ðŸ¤£ðŸ˜­",
         "1"
        ],
        [
         "3",
         "ongkosnya kecil,. kasiann drivernya.. GK ada otak dr pihak aplikasi Maxim..",
         "1"
        ],
        [
         "4",
         "Maps eror kalau ketujuan, supirnya gk ada sabar2nya",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content</td>\n",
       "      <td>score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maksa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tolong perbaiki mapsnya.maps ojol ko.kaya maps...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ongkosnya kecil,. kasiann drivernya.. GK ada o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maps eror kalau ketujuan, supirnya gk ada saba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0                                            content  score\n",
       "1                                              maksa      1\n",
       "2  tolong perbaiki mapsnya.maps ojol ko.kaya maps...      1\n",
       "3  ongkosnya kecil,. kasiann drivernya.. GK ada o...      1\n",
       "4  Maps eror kalau ketujuan, supirnya gk ada saba...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membaca file .csv hasil scraping ulasan\n",
    "df = pd.read_csv(\"combined_reviews.csv\", engine=\"python\", header=None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a1c409da-6aa1-432f-b1ef-9d2481749baa",
       "rows": [
        [
         "1",
         "maksa"
        ],
        [
         "2",
         "tolong perbaiki mapsnya.maps ojol ko.kaya maps gtaðŸ¤£ðŸ¤£ðŸ˜­"
        ],
        [
         "3",
         "ongkosnya kecil,. kasiann drivernya.. GK ada otak dr pihak aplikasi Maxim.."
        ],
        [
         "4",
         "Maps eror kalau ketujuan, supirnya gk ada sabar2nya"
        ],
        [
         "5",
         "baik"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maksa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tolong perbaiki mapsnya.maps ojol ko.kaya maps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ongkosnya kecil,. kasiann drivernya.. GK ada o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maps eror kalau ketujuan, supirnya gk ada saba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>baik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "1                                              maksa\n",
       "2  tolong perbaiki mapsnya.maps ojol ko.kaya maps...\n",
       "3  ongkosnya kecil,. kasiann drivernya.. GK ada o...\n",
       "4  Maps eror kalau ketujuan, supirnya gk ada saba...\n",
       "5                                               baik"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hapus kolom yang tidak diperlukan, yang diambil hanya \"content\"\n",
    "columns_to_drop = [1]\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Hapus baris pertama\n",
    "df = df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
    "\n",
    "# Simpan dataframe yang telah diubah ke dalam file .csv\n",
    "df.to_csv('review_content.csv', index=False)\n",
    "\n",
    "# Memanggil data\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# **2. Preprocessing Data**\n",
    "Pra-proses teks atau *text preprocessing adalah* serangkaian proses yang digunakan untuk menyiapkan teks mentah agar lebih siap untuk dianalisis. Tujuan utama *text preprocessing* adalah mengurangi kebisingan dan menyederhanakan teks sehingga analisis lanjutan atau algoritma pembelajaran mesin dapat bekerja lebih efisien dan efektif. Pada tahapan *text preprocessing* semua dilakukan dengan menggunakan *library* dari *Natural Language Toolkit* (NLTK), untuk memudahkan proses *text preprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memanggil model spaCy untuk tokenisasi\n",
    "nlp = spacy.blank(\"id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dua sel dibawah hanya berfungsi untuk mengecek dan menangani kasus baris dengan nilai NaN, yang dapat menyebabkan kode pemrosesan teks tidak mampu membaca baris tersebut, karena tidak memiliki datatype str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [content]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan baris untuk mengecek yang memiliki nilai NaN pada kolom 'content'\n",
    "print(df[df['content'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Menghapus baris dengan nilai NaN pada kolom 'content'\n",
    "df = df.dropna(subset=['content'])\n",
    "\n",
    "# Periksa kembali apakah masih ada nilai NaN\n",
    "print(df['content'].isna().sum())  # Output harus 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibawah didefinisikan kode untuk tahap-tahap text preprocessing. Adapun tahap-tahap yang dilakukan adalah:\n",
    "- **Text Cleaning**: proses menghapus tanda baca dan karakter yang tidak diperlukan seperti URL, angka, karakter spesial (seperti emoji), tanda baca, dan spasi berlebih.\n",
    "- **Case Folding**: proses dimana seluruh huruf dalam teks ulasan disesuaikan menjadi huruf kecil atau huruf besar, sehingga seluruh huruf dalam teks memiliki format penulisan yang seragam.\n",
    "- **Tokenization**: proses membagi teks atau dokumen menjadi unitâ€‘unit yang lebih kecil yang disebut â€tokenâ€. Token merujuk pada kataâ€‘kata individual dalam teks.\n",
    "- **Stopword Removal**: proses mengidentifikasi dan menghapus kataâ€‘kata umum yang tidak memberikan informasi signifikan dalam analisis sentimen.\n",
    "- **Stemming**: proses yang melibatkan penghilangan akhiran atau awalan kataâ€‘kata untuk menghasilkan bentuk dasar atau â€akarâ€ kata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleansing result to processed_data_cleansing.csv\n",
      "Saved case_folding result to processed_data_case_folding.csv\n",
      "Saved tokenization result to processed_data_tokenization.csv\n",
      "Saved stopword_removal result to processed_data_stopword_removal.csv\n",
      "Saved stemming result to processed_data_stemming.csv\n"
     ]
    }
   ],
   "source": [
    "# Kode per langkah preprocessing\n",
    "# Step 1: Cleansing text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Hapus URL\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    return text\n",
    "\n",
    "# Step 2: Case folding\n",
    "def case_folding(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Step 3: Tokenisasi\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc] \n",
    "\n",
    "# Step 4: Menghapus stopword menggunakan daftar dari Sastrawi\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_list = set(stopword_factory.get_stop_words())  \n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopword_list] \n",
    "\n",
    "# Step 5: Stemming menggunakan Sastrawi)\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "def stemming(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Fungsi untuk menyimpan hasil setiap langkah ke CSV\n",
    "def save_to_csv(df, step_name):\n",
    "    filename = f'processed_data_{step_name}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {step_name} result to {filename}\")\n",
    "\n",
    "# Fungsi utama untuk memproses dan menyimpan setiap langkah\n",
    "def process_and_save_steps(df):\n",
    "    # Step 1: Cleansing text\n",
    "    df['cleaned_text'] = df['content'].apply(clean_text)\n",
    "    save_to_csv(df[['content', 'cleaned_text']], 'cleansing')\n",
    "\n",
    "    # Step 2: Case folding\n",
    "    df['case_folding'] = df['cleaned_text'].apply(case_folding)\n",
    "    save_to_csv(df[['cleaned_text', 'case_folding']], 'case_folding')\n",
    "\n",
    "    # Step 3: Tokenisasi\n",
    "    df['tokenized_text'] = df['case_folding'].apply(tokenize)\n",
    "    save_to_csv(df[['case_folding', 'tokenized_text']], 'tokenization')\n",
    "\n",
    "    # Step 4: Penghapusan stopword\n",
    "    df['no_stopwords'] = df['tokenized_text'].apply(remove_stopwords)\n",
    "    save_to_csv(df[['tokenized_text', 'no_stopwords']], 'stopword_removal')\n",
    "\n",
    "    # Step 5: Stemming\n",
    "    df['stemmed_text'] = df['no_stopwords'].apply(stemming)\n",
    "    save_to_csv(df[['no_stopwords', 'stemmed_text']], 'stemming')\n",
    "\n",
    "# Menjalankan proses preprocessing\n",
    "df = pd.read_csv('review_content.csv')\n",
    "process_and_save_steps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                content  \\\n",
      "0                                                 maksa   \n",
      "1     tolong perbaiki mapsnya.maps ojol ko.kaya maps...   \n",
      "2     ongkosnya kecil,. kasiann drivernya.. GK ada o...   \n",
      "3     Maps eror kalau ketujuan, supirnya gk ada saba...   \n",
      "4                                                  baik   \n",
      "...                                                 ...   \n",
      "4995                                     bagus supirnya   \n",
      "4996                                   driver baik hati   \n",
      "4997                                             mantap   \n",
      "4998                                 asikk ramah banget   \n",
      "4999                   biarlah bintang yang membuktikan   \n",
      "\n",
      "                                           cleaned_text  \\\n",
      "0                                                 maksa   \n",
      "1      tolong perbaiki mapsnyamaps ojol kokaya maps gta   \n",
      "2     ongkosnya kecil kasiann drivernya GK ada otak ...   \n",
      "3     Maps eror kalau ketujuan supirnya gk ada sabarnya   \n",
      "4                                                  baik   \n",
      "...                                                 ...   \n",
      "4995                                     bagus supirnya   \n",
      "4996                                   driver baik hati   \n",
      "4997                                             mantap   \n",
      "4998                                 asikk ramah banget   \n",
      "4999                   biarlah bintang yang membuktikan   \n",
      "\n",
      "                                           case_folding  \\\n",
      "0                                                 maksa   \n",
      "1      tolong perbaiki mapsnyamaps ojol kokaya maps gta   \n",
      "2     ongkosnya kecil kasiann drivernya gk ada otak ...   \n",
      "3     maps eror kalau ketujuan supirnya gk ada sabarnya   \n",
      "4                                                  baik   \n",
      "...                                                 ...   \n",
      "4995                                     bagus supirnya   \n",
      "4996                                   driver baik hati   \n",
      "4997                                             mantap   \n",
      "4998                                 asikk ramah banget   \n",
      "4999                   biarlah bintang yang membuktikan   \n",
      "\n",
      "                                         tokenized_text  \\\n",
      "0                                               [maksa]   \n",
      "1     [tolong, perbaiki, mapsnyamaps, ojol, kokaya, ...   \n",
      "2     [ongkosnya, kecil, kasiann, drivernya, gk, ada...   \n",
      "3     [maps, eror, kalau, ketujuan, supirnya, gk, ad...   \n",
      "4                                                [baik]   \n",
      "...                                                 ...   \n",
      "4995                                  [bagus, supirnya]   \n",
      "4996                               [driver, baik, hati]   \n",
      "4997                                           [mantap]   \n",
      "4998                             [asikk, ramah, banget]   \n",
      "4999              [biarlah, bintang, yang, membuktikan]   \n",
      "\n",
      "                                           no_stopwords  \\\n",
      "0                                               [maksa]   \n",
      "1      [perbaiki, mapsnyamaps, ojol, kokaya, maps, gta]   \n",
      "2     [ongkosnya, kecil, kasiann, drivernya, gk, ota...   \n",
      "3     [maps, eror, kalau, ketujuan, supirnya, gk, sa...   \n",
      "4                                                [baik]   \n",
      "...                                                 ...   \n",
      "4995                                  [bagus, supirnya]   \n",
      "4996                               [driver, baik, hati]   \n",
      "4997                                           [mantap]   \n",
      "4998                             [asikk, ramah, banget]   \n",
      "4999                    [biarlah, bintang, membuktikan]   \n",
      "\n",
      "                                           stemmed_text  \n",
      "0                                               [maksa]  \n",
      "1          [baik, mapsnyamaps, ojol, kokaya, maps, gta]  \n",
      "2     [ongkos, kecil, kasiann, drivernya, gk, otak, ...  \n",
      "3        [maps, eror, kalau, tuju, supirnya, gk, sabar]  \n",
      "4                                                [baik]  \n",
      "...                                                 ...  \n",
      "4995                                  [bagus, supirnya]  \n",
      "4996                               [driver, baik, hati]  \n",
      "4997                                           [mantap]  \n",
      "4998                             [asikk, ramah, banget]  \n",
      "4999                             [biar, bintang, bukti]  \n",
      "\n",
      "[4886 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtering dan penghapusan baris dengan \"processed_text\" yang kosong (untuk kode per langkah)\n",
    "# Hapus baris yang kosong setelah preprocessing (cek apakah list kosong)\n",
    "df_filtered_full = df[df['stemmed_text'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Tampilkan hasil setelah filtering\n",
    "print(df_filtered_full)\n",
    "\n",
    "# Simpan hasil semua tahap text preprocessing ke file CSV\n",
    "df_filtered_full.to_csv('filtered_data_full.csv', index=False)\n",
    "\n",
    "# Simpan hanya ulasan awal ('content') dan hasil tokenisasi ke file CSV\n",
    "columns_to_drop = ['cleaned_text', 'case_folding', 'no_stopwords']\n",
    "df_filtered = df_filtered_full.drop(columns=columns_to_drop)\n",
    "\n",
    "# Ganti nama kolom hasil akhir agar sesuai\n",
    "df_filtered = df_filtered.rename(columns={'stemmed_text': 'processed_text'})\n",
    "\n",
    "# Simpan dataframe yang telah diubah ke dalam file .csv\n",
    "df_filtered.to_csv('filtered_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# **3. Pelabelan Data**\n",
    "Proses pelabelan dilakukan dengan memberi label pada data ulasan, untuk mengetahui apakah suatu ulasan memiliki sentimen positif atau negatif. Pelabelan dilakukan secara otomatis dengan menggunakan metode *lexicon-based*, dengan menggunakan kamus Valence Aware Dictionary for Sentiment Reasoning (VADER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengunduh leksikon VADER\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, akan didefinisikan fungsi untuk menerjemahkan teks dari bahasa Indonesia ke bahasa Inggris, karena VADER menggunakan bahasa Inggris dalam pelabelannya. VADER akan mengembalikan nilai compound berdasarkan kata-kata dalam suatu teks, dan dari nilai ini kita bisa menentukan label untuk data. Untuk negatif, compound < -0.05; dan untuk positif > 0.05, dengan nilai compound di antaranya menandakan sentimen netral, yang tidak dipakai dalam penelitian ini dan akan dihapus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat objek Google Translate dan VADER untuk pelabelan data (0.05)\n",
    "translator = Translator()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Fungsi untuk menerjemahkan teks dari bahasa Indonesia ke bahasa Inggris\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        translated = translator.translate(text, src='id', dest='en')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in translation: {e}\")\n",
    "        return text\n",
    "\n",
    "# Fungsi untuk pelabelan sentimen otomatis menggunakan VADER\n",
    "def vader_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    if scores['compound'] >= 0.05:\n",
    "        return 'positif'  \n",
    "    elif scores['compound'] <= -0.05:\n",
    "        return 'negatif'  \n",
    "    else:\n",
    "        return None \n",
    "\n",
    "# Menyimpan ulasan yang valid (tidak netral)\n",
    "filtered_reviews = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode dibawah menerapkan fungsi yang telah didefinisikan pada data ulasan. Kolom content berisi ulasan asli, processed_text berisi ulasan yang sudah diproses, translated_text berisi processed_text yang sudah diterjemahkan ke bahasa Inggris, dan vader_sentiment adalah label berupa sentimen positif atau negatif berdasarkan skor compound VADER, yang tidak ditampilkan pada tahap ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         processed_text  \\\n",
      "1371  ['gimna', 'kok', 'ngisi', 'kaspro', 'malah', '...   \n",
      "199   ['maxim', 'update', 'sm', 'titik', 'lokasi', '...   \n",
      "2194  ['balikin', 'saldo', 'woyyy', 'baru', 'topup',...   \n",
      "501   ['kemudi', 'nya', 'kasar', 'bgt', 'katakatanya...   \n",
      "811   ['aplikasi', 'nya', 'sangat', 'cari', 'untungs...   \n",
      "2825  ['layan', 'prima', 'orang', 'ramah', 'kendara'...   \n",
      "1221  ['cross', 'cek', 'driver', 'atas', 'nama', 'no...   \n",
      "\n",
      "                                        translated_text vader_sentiment  \n",
      "1371  ['Gimna', 'How' how ',' filling ',' kaspro ','...         negatif  \n",
      "199   ['maxim', 'update', 'sm', 'point', 'location',...         negatif  \n",
      "2194  ['return', 'balance', 'woyyy', 'new', 'topup',...         positif  \n",
      "501   ['steering', '' rough ',' rough ',' bgt ',' bg...         negatif  \n",
      "811   ['application', '' very ',' very ',' search ',...         positif  \n",
      "2825  ['service', 'prime', 'person', 'friendly', 've...         positif  \n",
      "1221  ['cross',' check ',' driver ',' above ',' name...         negatif  \n"
     ]
    }
   ],
   "source": [
    "# Kode untuk sampel, menguji waktu running\n",
    "# Misalkan df adalah DataFrame yang sudah ada dengan kolom \"processed_text\"\n",
    "df = pd.read_csv('filtered_data.csv')\n",
    "\n",
    "# Step 2: Ambil hanya n baris data untuk uji cepat\n",
    "df_sample = df.sample(n=100, random_state=42) \n",
    "\n",
    "# Step 3: Menerjemahkan kolom \"processed_text\" ke bahasa Inggris\n",
    "df_sample['translated_text'] = df_sample['processed_text'].apply(translate_text)\n",
    "\n",
    "# Step 4: Pelabelan otomatis menggunakan VADER pada teks yang sudah diterjemahkan\n",
    "df_sample['vader_sentiment'] = df_sample['translated_text'].apply(vader_sentiment)\n",
    "\n",
    "# Step 5: Hapus baris dengan nilai None di vader_sentiment\n",
    "df_sample = df_sample.dropna(subset=['vader_sentiment'])\n",
    "\n",
    "# Tampilkan hasil setelah pelabelan\n",
    "print(df_sample[['processed_text', 'translated_text', 'vader_sentiment']])\n",
    "\n",
    "# Simpan hasil pelabelan ke file CSV baru (untuk data uji coba ini)\n",
    "df_sample.to_csv('vader_labeled_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in translation: The read operation timed out\n",
      "                                         processed_text  \\\n",
      "0         ['ongkos', 'manusia', 'manusia', 'drivernya']   \n",
      "1                                              ['baik']   \n",
      "2                                       ['terimakasih']   \n",
      "3                        ['aplikasi', 'susah', 'pakai']   \n",
      "4     ['lah', 'management', 'pihak', 'maxim', 'baik'...   \n",
      "...                                                 ...   \n",
      "9704                           ['pelayananya', 'goood']   \n",
      "9705                                         ['mantap']   \n",
      "9706                                 ['sangat', 'baik']   \n",
      "9707                                         ['nyaman']   \n",
      "9708  ['order', 'penuh', 'roy', 'delmax', 'vin', 'h'...   \n",
      "\n",
      "                                        translated_text vader_sentiment  \n",
      "0                  ['cost', 'human', 'human', 'driver']            None  \n",
      "1                                              ['Good']            None  \n",
      "2                                         ['Thank You']            None  \n",
      "3                   ['application', 'difficult', 'use']            None  \n",
      "4     ['lah', 'management', 'party', 'maxim', 'good'...         negatif  \n",
      "...                                                 ...             ...  \n",
      "9704                                ['service', 'good']            None  \n",
      "9705                                      ['Excellent']            None  \n",
      "9706                                      ['Very good']            None  \n",
      "9707                                    ['comfortable']            None  \n",
      "9708  ['Order', 'Full', 'Roy', 'Delmax', 'Vin', 'H',...         positif  \n",
      "\n",
      "[9709 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Kode untuk dataset full\n",
    "# Misalkan df adalah DataFrame yang sudah ada dengan kolom \"processed_text\"\n",
    "df = pd.read_csv('filtered_data.csv') \n",
    "\n",
    "# Step 1: Menerjemahkan kolom \"processed_text\" ke bahasa Inggris\n",
    "df['translated_text'] = df['processed_text'].apply(translate_text)\n",
    "\n",
    "# Step 2: Pelabelan otomatis menggunakan VADER pada teks yang sudah diterjemahkan\n",
    "df['vader_sentiment'] = df['translated_text'].apply(vader_sentiment)\n",
    "\n",
    "# Tampilkan hasil setelah pelabelan\n",
    "print(df[['processed_text', 'translated_text', 'vader_sentiment']])\n",
    "\n",
    "# Simpan hasil pelabelan ke file CSV baru\n",
    "df.to_csv('vader_labeled_data.csv', index=False)\n",
    "# Waktu running: 89m 19.9s (4868 baris)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada sel ini, dilakukan konversi format pada data di kolom translated_text dari yang sebelumnya menyerupai list menjadi string biasa tanpa koma pemisah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      processed_text  \\\n",
      "0  ['gimna', 'kok', 'ngisi', 'kaspro', 'malah', '...   \n",
      "1  ['maxim', 'update', 'sm', 'titik', 'lokasi', '...   \n",
      "2  ['balikin', 'saldo', 'woyyy', 'baru', 'topup',...   \n",
      "3  ['kemudi', 'nya', 'kasar', 'bgt', 'katakatanya...   \n",
      "4  ['aplikasi', 'nya', 'sangat', 'cari', 'untungs...   \n",
      "\n",
      "                                     translated_text vader_sentiment  compound  \n",
      "0  ['Gimna', 'How' how ',' filling ',' kaspro ','...         negatif   -0.1280  \n",
      "1  ['maxim', 'update', 'sm', 'point', 'location',...         negatif   -0.2023  \n",
      "2  ['return', 'balance', 'woyyy', 'new', 'topup',...         positif    0.4019  \n",
      "3  ['steering', '' rough ',' rough ',' bgt ',' bg...         negatif   -0.5719  \n",
      "4  ['application', '' very ',' very ',' search ',...         positif    0.7220  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Baca file CSV yang sudah ada\n",
    "df = pd.read_csv('vader_labeled_data.csv')\n",
    "\n",
    "# Step 2: Bersihkan karakter yang tidak diperlukan\n",
    "df['converted_text'] = df['translated_text'].str.replace(r\"[\\[\\]',]\", '', regex=True)\n",
    "\n",
    "# Step 3: Lakukan pelabelan sentimen dengan VADER pada kolom 'converted_text'\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Fungsi untuk pelabelan sentimen otomatis menggunakan skor compound VADER\n",
    "def vader_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    if scores['compound'] > 0.05:\n",
    "        sentiment = 'positif'\n",
    "    elif scores['compound'] < -0.05:\n",
    "        sentiment = 'negatif'\n",
    "    else:\n",
    "        sentiment = None \n",
    "    \n",
    "    return sentiment, scores['compound']\n",
    "\n",
    "# Terapkan pelabelan dan simpan skor compound VADER di kolom terpisah\n",
    "df['vader_sentiment'], df['compound'] = zip(*df['converted_text'].apply(vader_sentiment))\n",
    "\n",
    "# Step 4: Hapus ulasan netral (dengan sentimen None)\n",
    "df = df.dropna(subset=['vader_sentiment'])\n",
    "\n",
    "# Step 5: Simpan hasil pelabelan dan skor compound ke file CSV baru\n",
    "df[['processed_text', 'translated_text', 'converted_text', 'vader_sentiment', 'compound']].to_csv('vader_labeled_with_compound.csv', index=False)\n",
    "\n",
    "# Tampilkan beberapa hasil\n",
    "print(df[['processed_text', 'translated_text', 'vader_sentiment', 'compound']].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# **4. Pembagian Data**\n",
    "Pada tahap pembagian data, data akan dibagi menjadi data latih, data validasi, dan data uji dengan proporsi 60:20:20, 70:25:5, 80:10:10. Pada data latih akan dilatih model yang akan digunakan, pada data validasi akan dievaluasi performa model selama pelatihan untuk memantau overfitting dan menyesuaikan hyperparameter, dan kemudian model yang telah dilatih akan digunakan pada data uji untuk melakukan analisis sentimen dan mengukur performa akhir model pada data baru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data latihan: 6278\n",
      "Jumlah data validasi: 785\n",
      "Jumlah data uji: 785\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk mengubah sentimen menjadi angka\n",
    "def sentiment_to_float(label):\n",
    "    if label == 'positif':\n",
    "        return 1.0\n",
    "    elif label == 'negatif':\n",
    "        return 0.0\n",
    "\n",
    "# Step 1: Baca CSV dan hanya pilih kolom yang dibutuhkan (processed_text dan vader_sentiment)\n",
    "df = pd.read_csv(\"vader_labeled_with_compound.csv\", usecols=['processed_text', 'vader_sentiment'])\n",
    "\n",
    "# Simpan CSV baru yang hanya berisi kolom yang dibutuhkan\n",
    "df.to_csv('filtered_vader_labeled.csv', index=False)\n",
    "\n",
    "# Step 2: Mendefinisikan Field untuk processed_text dan vader_sentiment\n",
    "TEXT = data.Field(tokenize=lambda x: eval(x), sequential=True, lower=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float, preprocessing=data.Pipeline(sentiment_to_float))\n",
    "\n",
    "# Step 3: Memetakan data ke fields sesuai dengan CSV yang sudah difilter\n",
    "fields = [('processed_text', TEXT), ('vader_sentiment', LABEL)]\n",
    "\n",
    "# Step 4: Load data dari CSV yang sudah difilter\n",
    "dataset = data.TabularDataset(\n",
    "    path=\"filtered_vader_labeled.csv\", \n",
    "    format=\"csv\",\n",
    "    fields=fields,\n",
    "    skip_header=True \n",
    ")\n",
    "\n",
    "# Step 5: Bagi data menjadi dataset train, validation, dan test\n",
    "# Uji satu-satu proporsi pembagian data\n",
    "# train_data, test_data, valid_data = dataset.split(split_ratio=[0.6, 0.2, 0.2])\n",
    "# train_data, test_data, valid_data = dataset.split(split_ratio=[0.7, 0.25, 0.05])\n",
    "train_data, test_data, valid_data = dataset.split(split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "# Step 6: Print jumlah sampel dalam setiap dataset\n",
    "print(f\"Jumlah data latihan: {len(train_data)}\")\n",
    "print(f\"Jumlah data validasi: {len(valid_data)}\")\n",
    "print(f\"Jumlah data uji: {len(test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode dibawah berfungsi untuk menyimpan data yang sudah dibagi ke dalam file nya masing-masing. Adapun pembagian data dengan metode split dilakukan secara acak dan bukan berdasarkan indeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data disimpan ke train_data.csv\n",
      "Data disimpan ke valid_data.csv\n",
      "Data disimpan ke test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk menyimpan dataset ke dalam file CSV\n",
    "def save_dataset_to_csv(dataset, filename):\n",
    "    data = {\n",
    "        'processed_text': [\" \".join(vars(example)['processed_text']) for example in dataset.examples],\n",
    "        'vader_sentiment': [vars(example)['vader_sentiment'] for example in dataset.examples]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data disimpan ke {filename}\")\n",
    "\n",
    "# Menyimpan masing-masing subset ke file CSV\n",
    "save_dataset_to_csv(train_data, 'train_data.csv')\n",
    "save_dataset_to_csv(valid_data, 'valid_data.csv')\n",
    "save_dataset_to_csv(test_data, 'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processed_text': ['jelek'], 'vader_sentiment': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Memanggil contoh dari data training\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# **5. Ekstraksi Fitur fastText**\n",
    "Pada tahap ekstraksi fitur fastText, akan dibangun kosakata untuk data latih menggunakan model word embedding fastText. Setiap token dari data latih akan direpresentasikan dalam bentuk vektor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Membangun Kosakata (Vocabulary)**\n",
    "Membangun kosakata untuk dataset training menggunakan embedding fastText yang telah dilatih sebelumnya (*pre-trained*). Word vector yang digunakan berdimensi 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token yang paling sering muncul:\n",
      "[('driver', 1516), ('baik', 1271), ('nya', 1250), ('maxim', 1164), ('aplikasi', 937), ('sangat', 799), ('ramah', 754), ('yg', 735), ('gak', 691), ('mau', 665)]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tentukan ukuran maksimal kosakata\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "# Step 2: Muat model fastText yang telah dikompresi\n",
    "ft_compressed = compress_fasttext.models.CompressedFastTextKeyedVectors.load('fasttext-id-mini')\n",
    "\n",
    "# Step 3: Buat kosakata berdasarkan data training\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "# Step 4: Buat tensor embedding untuk setiap kata di dalam kosakata\n",
    "embedding_matrix = torch.zeros((len(TEXT.vocab), ft_compressed.vector_size))\n",
    "\n",
    "for word, idx in TEXT.vocab.stoi.items():  \n",
    "    if word in ft_compressed.key_to_index:  \n",
    "        embedding_matrix[idx] = torch.tensor(ft_compressed[word]) \n",
    "\n",
    "# Step 5: Simpan embedding matrix ke TEXT.vocab\n",
    "TEXT.vocab.set_vectors(stoi=TEXT.vocab.stoi, vectors=embedding_matrix, dim=ft_compressed.vector_size)\n",
    "\n",
    "# Step 6: Membangun kosakata untuk label (LABEL)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Step 7: Simpan token dan frekuensi ke file CSV\n",
    "vocab_data = [(token, freq) for token, freq in TEXT.vocab.freqs.items()]\n",
    "\n",
    "# Konversi ke DataFrame\n",
    "vocab_df = pd.DataFrame(vocab_data, columns=['Token', 'Frequency'])\n",
    "\n",
    "# Simpan ke file CSV\n",
    "vocab_df.to_csv('vocab_frequencies.csv', index=False)\n",
    "\n",
    "# Step 8: Tampilkan 10 token yang paling sering muncul\n",
    "print(\"Token yang paling sering muncul:\")\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "braesWjckkJu"
   },
   "source": [
    "<a id='section6'></a>\n",
    "# **6. Pembentukan dan Pelatihan Model BiLSTM**\n",
    "Pada tahap pembentukan model BiLSTM, akan didefinisikan model BiLSTM dan berbagai hyperparameternya. Setelah itu, akan diganti bobot awal layer embedding dengan embedding yang sudah dilatih sebelumnya pada tahap pembentukan model word embedding fastText. Kemudian, dilakukan pengujian pada setiap kombinasi nilai hyperparameter yang mungkin (Grid Search), dimana kombinasi yang memiliki nilai f1-score paling tinggi akan disimpan sebagai model terbaik dan akan digunakan pada tahap pengujian pada data uji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Membuat Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Layer Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # 2. Layer LSTM\n",
    "        self.encoder = nn.LSTM(embedding_dim,               # Ukuran input adalah embedding_dim\n",
    "                               hidden_dim,                  # Ukuran hidden state\n",
    "                               num_layers=n_layers,         # Jumlah lapisan LSTM\n",
    "                               bidirectional=bidirectional, # Menentukan arah LSTM\n",
    "                               dropout=dropout)             # Dropout antar lapisan\n",
    "\n",
    "        # 3. Fully-connected Layer\n",
    "        self.predictor = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        # Layer dropout untuk regularisasi, mencegah overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        # Menghasilkan embedding dari input text, dengan dropout untuk regularisasi\n",
    "        embedded = self.dropout(self.embedding(text))    \n",
    "\n",
    "        # Menggunakan pack_padded_sequence untuk melewati token padding dalam RNN, mempercepat komputasi\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "\n",
    "        # LSTM menghasilkan packed_output, dan tuple (hidden state terakhir, cell state terakhir)\n",
    "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
    "\n",
    "        # Mengembalikan packed sequence menjadi tensor\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        # Mengambil hidden state dari lapisan terakhir untuk arah forward dan backward\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "\n",
    "        # Menerapkan layer fully-connected untuk menghasilkan prediksi\n",
    "        return self.predictor(hidden)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAk1scwU_g3d"
   },
   "source": [
    "### **Melatih Model**\n",
    "Pada tahap ini akan ditentukan rentang nilai hyperparameter yang akan diuji dan setiap kombinasi yang mungkin akan diterapkan pada model dan diuji. Pada setiap model, epoch dengan validation loss terendah akan diambil, dan akan dibandingkan dengan model lainnya untuk menentukan mana yang menunjukkan performa terbaik pada dataset validasi, berdasarkan metrik f1-score yang tertinggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menguji kombinasi: hidden_dim=64, batch_size=32, dropout=0.3, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 26s\n",
      "\t Train Loss 0.379 | Train F1-score: 0.702\n",
      "\t Validation Loss 0.253 | Validation F1-score: 0.8\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.167 | Train F1-score: 0.9\n",
      "\t Validation Loss 0.273 | Validation F1-score: 0.819\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.09 | Train F1-score: 0.951\n",
      "\t Validation Loss 0.292 | Validation F1-score: 0.838\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.053 | Train F1-score: 0.972\n",
      "\t Validation Loss 0.443 | Validation F1-score: 0.831\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.033 | Train F1-score: 0.982\n",
      "\t Validation Loss 0.423 | Validation F1-score: 0.823\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.032 | Train F1-score: 0.98\n",
      "\t Validation Loss 0.475 | Validation F1-score: 0.83\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.016 | Train F1-score: 0.991\n",
      "\t Validation Loss 0.421 | Validation F1-score: 0.81\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 26s\n",
      "\t Train Loss 0.015 | Train F1-score: 0.992\n",
      "\t Validation Loss 0.476 | Validation F1-score: 0.824\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 20s\n",
      "\t Train Loss 0.013 | Train F1-score: 0.993\n",
      "\t Validation Loss 0.529 | Validation F1-score: 0.798\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 26s\n",
      "\t Train Loss 0.005 | Train F1-score: 0.998\n",
      "\t Validation Loss 0.735 | Validation F1-score: 0.825\n",
      "Best Validation Loss: 0.253\n",
      "Validation Accuracy at Best Loss: 0.880\n",
      "Validation F1-score: 0.800\n",
      "Menguji kombinasi: hidden_dim=64, batch_size=64, dropout=0.3, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 20s\n",
      "\t Train Loss 0.469 | Train F1-score: 0.615\n",
      "\t Validation Loss 0.294 | Validation F1-score: 0.802\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 21s\n",
      "\t Train Loss 0.192 | Train F1-score: 0.881\n",
      "\t Validation Loss 0.284 | Validation F1-score: 0.825\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.107 | Train F1-score: 0.942\n",
      "\t Validation Loss 0.27 | Validation F1-score: 0.836\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 16s\n",
      "\t Train Loss 0.059 | Train F1-score: 0.972\n",
      "\t Validation Loss 0.334 | Validation F1-score: 0.831\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 16s\n",
      "\t Train Loss 0.037 | Train F1-score: 0.982\n",
      "\t Validation Loss 0.368 | Validation F1-score: 0.831\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 15s\n",
      "\t Train Loss 0.022 | Train F1-score: 0.989\n",
      "\t Validation Loss 0.45 | Validation F1-score: 0.829\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.029 | Train F1-score: 0.984\n",
      "\t Validation Loss 0.356 | Validation F1-score: 0.826\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 20s\n",
      "\t Train Loss 0.021 | Train F1-score: 0.988\n",
      "\t Validation Loss 0.482 | Validation F1-score: 0.812\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.01 | Train F1-score: 0.996\n",
      "\t Validation Loss 0.585 | Validation F1-score: 0.838\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.009 | Train F1-score: 0.994\n",
      "\t Validation Loss 0.467 | Validation F1-score: 0.819\n",
      "Best Validation Loss: 0.270\n",
      "Validation Accuracy at Best Loss: 0.889\n",
      "Validation F1-score: 0.836\n",
      "Menguji kombinasi: hidden_dim=64, batch_size=32, dropout=0.3, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 19s\n",
      "\t Train Loss 0.662 | Train F1-score: 0.0\n",
      "\t Validation Loss 0.626 | Validation F1-score: 0.0\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 19s\n",
      "\t Train Loss 0.465 | Train F1-score: 0.6\n",
      "\t Validation Loss 0.362 | Validation F1-score: 0.735\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 19s\n",
      "\t Train Loss 0.29 | Train F1-score: 0.824\n",
      "\t Validation Loss 0.308 | Validation F1-score: 0.794\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.216 | Train F1-score: 0.876\n",
      "\t Validation Loss 0.284 | Validation F1-score: 0.828\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.165 | Train F1-score: 0.915\n",
      "\t Validation Loss 0.275 | Validation F1-score: 0.838\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 20s\n",
      "\t Train Loss 0.132 | Train F1-score: 0.934\n",
      "\t Validation Loss 0.263 | Validation F1-score: 0.857\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.106 | Train F1-score: 0.949\n",
      "\t Validation Loss 0.258 | Validation F1-score: 0.873\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 21s\n",
      "\t Train Loss 0.086 | Train F1-score: 0.96\n",
      "\t Validation Loss 0.265 | Validation F1-score: 0.869\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.071 | Train F1-score: 0.969\n",
      "\t Validation Loss 0.266 | Validation F1-score: 0.863\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 20s\n",
      "\t Train Loss 0.063 | Train F1-score: 0.973\n",
      "\t Validation Loss 0.267 | Validation F1-score: 0.867\n",
      "Best Validation Loss: 0.258\n",
      "Validation Accuracy at Best Loss: 0.920\n",
      "Validation F1-score: 0.873\n",
      "Menguji kombinasi: hidden_dim=64, batch_size=64, dropout=0.3, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 15s\n",
      "\t Train Loss 0.694 | Train F1-score: 0.185\n",
      "\t Validation Loss 0.68 | Validation F1-score: 0.025\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 15s\n",
      "\t Train Loss 0.659 | Train F1-score: 0.01\n",
      "\t Validation Loss 0.62 | Validation F1-score: 0.026\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.494 | Train F1-score: 0.583\n",
      "\t Validation Loss 0.405 | Validation F1-score: 0.722\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.323 | Train F1-score: 0.82\n",
      "\t Validation Loss 0.344 | Validation F1-score: 0.77\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 15s\n",
      "\t Train Loss 0.249 | Train F1-score: 0.869\n",
      "\t Validation Loss 0.343 | Validation F1-score: 0.757\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.2 | Train F1-score: 0.895\n",
      "\t Validation Loss 0.311 | Validation F1-score: 0.807\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.163 | Train F1-score: 0.919\n",
      "\t Validation Loss 0.293 | Validation F1-score: 0.828\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.132 | Train F1-score: 0.935\n",
      "\t Validation Loss 0.316 | Validation F1-score: 0.81\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.111 | Train F1-score: 0.951\n",
      "\t Validation Loss 0.284 | Validation F1-score: 0.862\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.097 | Train F1-score: 0.959\n",
      "\t Validation Loss 0.297 | Validation F1-score: 0.831\n",
      "Best Validation Loss: 0.284\n",
      "Validation Accuracy at Best Loss: 0.903\n",
      "Validation F1-score: 0.862\n",
      "Menguji kombinasi: hidden_dim=64, batch_size=32, dropout=0.5, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.387 | Train F1-score: 0.713\n",
      "\t Validation Loss 0.269 | Validation F1-score: 0.794\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.176 | Train F1-score: 0.895\n",
      "\t Validation Loss 0.253 | Validation F1-score: 0.843\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.101 | Train F1-score: 0.95\n",
      "\t Validation Loss 0.268 | Validation F1-score: 0.841\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 16s\n",
      "\t Train Loss 0.065 | Train F1-score: 0.966\n",
      "\t Validation Loss 0.337 | Validation F1-score: 0.849\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.041 | Train F1-score: 0.978\n",
      "\t Validation Loss 0.294 | Validation F1-score: 0.847\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.032 | Train F1-score: 0.982\n",
      "\t Validation Loss 0.362 | Validation F1-score: 0.839\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.028 | Train F1-score: 0.983\n",
      "\t Validation Loss 0.381 | Validation F1-score: 0.828\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 19s\n",
      "\t Train Loss 0.015 | Train F1-score: 0.992\n",
      "\t Validation Loss 0.463 | Validation F1-score: 0.83\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.013 | Train F1-score: 0.993\n",
      "\t Validation Loss 0.495 | Validation F1-score: 0.813\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.028 | Train F1-score: 0.984\n",
      "\t Validation Loss 0.522 | Validation F1-score: 0.832\n",
      "Best Validation Loss: 0.253\n",
      "Validation Accuracy at Best Loss: 0.899\n",
      "Validation F1-score: 0.843\n",
      "Menguji kombinasi: hidden_dim=64, batch_size=64, dropout=0.5, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.495 | Train F1-score: 0.573\n",
      "\t Validation Loss 0.305 | Validation F1-score: 0.798\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.221 | Train F1-score: 0.867\n",
      "\t Validation Loss 0.266 | Validation F1-score: 0.83\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.13 | Train F1-score: 0.927\n",
      "\t Validation Loss 0.263 | Validation F1-score: 0.838\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.076 | Train F1-score: 0.961\n",
      "\t Validation Loss 0.293 | Validation F1-score: 0.851\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.049 | Train F1-score: 0.977\n",
      "\t Validation Loss 0.353 | Validation F1-score: 0.845\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.036 | Train F1-score: 0.982\n",
      "\t Validation Loss 0.38 | Validation F1-score: 0.84\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.039 | Train F1-score: 0.977\n",
      "\t Validation Loss 0.314 | Validation F1-score: 0.833\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.026 | Train F1-score: 0.986\n",
      "\t Validation Loss 0.389 | Validation F1-score: 0.828\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.015 | Train F1-score: 0.992\n",
      "\t Validation Loss 0.488 | Validation F1-score: 0.834\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.016 | Train F1-score: 0.991\n",
      "\t Validation Loss 0.429 | Validation F1-score: 0.821\n",
      "Best Validation Loss: 0.263\n",
      "Validation Accuracy at Best Loss: 0.893\n",
      "Validation F1-score: 0.838\n",
      "Menguji kombinasi: hidden_dim=64, batch_size=32, dropout=0.5, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.687 | Train F1-score: 0.251\n",
      "\t Validation Loss 0.66 | Validation F1-score: 0.0\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.558 | Train F1-score: 0.342\n",
      "\t Validation Loss 0.4 | Validation F1-score: 0.715\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.332 | Train F1-score: 0.796\n",
      "\t Validation Loss 0.32 | Validation F1-score: 0.792\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.256 | Train F1-score: 0.85\n",
      "\t Validation Loss 0.292 | Validation F1-score: 0.802\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.205 | Train F1-score: 0.889\n",
      "\t Validation Loss 0.283 | Validation F1-score: 0.828\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.169 | Train F1-score: 0.907\n",
      "\t Validation Loss 0.264 | Validation F1-score: 0.843\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 17s\n",
      "\t Train Loss 0.141 | Train F1-score: 0.926\n",
      "\t Validation Loss 0.258 | Validation F1-score: 0.859\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 18s\n",
      "\t Train Loss 0.119 | Train F1-score: 0.94\n",
      "\t Validation Loss 0.257 | Validation F1-score: 0.869\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 20s\n",
      "\t Train Loss 0.106 | Train F1-score: 0.944\n",
      "\t Validation Loss 0.252 | Validation F1-score: 0.869\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 19s\n",
      "\t Train Loss 0.092 | Train F1-score: 0.957\n",
      "\t Validation Loss 0.26 | Validation F1-score: 0.867\n",
      "Best Validation Loss: 0.252\n",
      "Validation Accuracy at Best Loss: 0.919\n",
      "Validation F1-score: 0.869\n",
      "Menguji kombinasi: hidden_dim=64, batch_size=64, dropout=0.5, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.694 | Train F1-score: 0.223\n",
      "\t Validation Loss 0.682 | Validation F1-score: 0.0\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.669 | Train F1-score: 0.013\n",
      "\t Validation Loss 0.643 | Validation F1-score: 0.0\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.564 | Train F1-score: 0.368\n",
      "\t Validation Loss 0.449 | Validation F1-score: 0.667\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.366 | Train F1-score: 0.782\n",
      "\t Validation Loss 0.361 | Validation F1-score: 0.771\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.292 | Train F1-score: 0.837\n",
      "\t Validation Loss 0.338 | Validation F1-score: 0.785\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 14s\n",
      "\t Train Loss 0.243 | Train F1-score: 0.866\n",
      "\t Validation Loss 0.319 | Validation F1-score: 0.8\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.204 | Train F1-score: 0.891\n",
      "\t Validation Loss 0.302 | Validation F1-score: 0.816\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.178 | Train F1-score: 0.907\n",
      "\t Validation Loss 0.299 | Validation F1-score: 0.805\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.157 | Train F1-score: 0.92\n",
      "\t Validation Loss 0.289 | Validation F1-score: 0.83\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.136 | Train F1-score: 0.932\n",
      "\t Validation Loss 0.285 | Validation F1-score: 0.813\n",
      "Best Validation Loss: 0.285\n",
      "Validation Accuracy at Best Loss: 0.880\n",
      "Validation F1-score: 0.813\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=32, dropout=0.3, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.361 | Train F1-score: 0.746\n",
      "\t Validation Loss 0.247 | Validation F1-score: 0.816\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.159 | Train F1-score: 0.905\n",
      "\t Validation Loss 0.248 | Validation F1-score: 0.825\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.082 | Train F1-score: 0.959\n",
      "\t Validation Loss 0.26 | Validation F1-score: 0.837\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 30s\n",
      "\t Train Loss 0.048 | Train F1-score: 0.975\n",
      "\t Validation Loss 0.342 | Validation F1-score: 0.836\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.032 | Train F1-score: 0.984\n",
      "\t Validation Loss 0.331 | Validation F1-score: 0.834\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 30s\n",
      "\t Train Loss 0.035 | Train F1-score: 0.98\n",
      "\t Validation Loss 0.476 | Validation F1-score: 0.799\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.03 | Train F1-score: 0.986\n",
      "\t Validation Loss 0.364 | Validation F1-score: 0.814\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 31s\n",
      "\t Train Loss 0.011 | Train F1-score: 0.994\n",
      "\t Validation Loss 0.5 | Validation F1-score: 0.824\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.008 | Train F1-score: 0.996\n",
      "\t Validation Loss 0.607 | Validation F1-score: 0.816\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.007 | Train F1-score: 0.996\n",
      "\t Validation Loss 0.644 | Validation F1-score: 0.826\n",
      "Best Validation Loss: 0.247\n",
      "Validation Accuracy at Best Loss: 0.887\n",
      "Validation F1-score: 0.816\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=64, dropout=0.3, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 26s\n",
      "\t Train Loss 0.45 | Train F1-score: 0.62\n",
      "\t Validation Loss 0.286 | Validation F1-score: 0.82\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.217 | Train F1-score: 0.868\n",
      "\t Validation Loss 0.264 | Validation F1-score: 0.843\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 26s\n",
      "\t Train Loss 0.111 | Train F1-score: 0.936\n",
      "\t Validation Loss 0.288 | Validation F1-score: 0.812\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 24s\n",
      "\t Train Loss 0.063 | Train F1-score: 0.969\n",
      "\t Validation Loss 0.297 | Validation F1-score: 0.832\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.041 | Train F1-score: 0.977\n",
      "\t Validation Loss 0.361 | Validation F1-score: 0.828\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.027 | Train F1-score: 0.986\n",
      "\t Validation Loss 0.353 | Validation F1-score: 0.836\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.024 | Train F1-score: 0.987\n",
      "\t Validation Loss 0.375 | Validation F1-score: 0.825\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.012 | Train F1-score: 0.993\n",
      "\t Validation Loss 0.518 | Validation F1-score: 0.834\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 24s\n",
      "\t Train Loss 0.012 | Train F1-score: 0.994\n",
      "\t Validation Loss 0.757 | Validation F1-score: 0.819\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.092 | Train F1-score: 0.959\n",
      "\t Validation Loss 0.429 | Validation F1-score: 0.829\n",
      "Best Validation Loss: 0.264\n",
      "Validation Accuracy at Best Loss: 0.900\n",
      "Validation F1-score: 0.843\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=32, dropout=0.3, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.639 | Train F1-score: 0.162\n",
      "\t Validation Loss 0.475 | Validation F1-score: 0.598\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 30s\n",
      "\t Train Loss 0.361 | Train F1-score: 0.757\n",
      "\t Validation Loss 0.328 | Validation F1-score: 0.77\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.254 | Train F1-score: 0.849\n",
      "\t Validation Loss 0.284 | Validation F1-score: 0.802\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.186 | Train F1-score: 0.898\n",
      "\t Validation Loss 0.272 | Validation F1-score: 0.834\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.135 | Train F1-score: 0.932\n",
      "\t Validation Loss 0.26 | Validation F1-score: 0.843\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 31s\n",
      "\t Train Loss 0.111 | Train F1-score: 0.945\n",
      "\t Validation Loss 0.246 | Validation F1-score: 0.857\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.087 | Train F1-score: 0.958\n",
      "\t Validation Loss 0.236 | Validation F1-score: 0.87\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 30s\n",
      "\t Train Loss 0.077 | Train F1-score: 0.961\n",
      "\t Validation Loss 0.245 | Validation F1-score: 0.87\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.057 | Train F1-score: 0.971\n",
      "\t Validation Loss 0.271 | Validation F1-score: 0.868\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 30s\n",
      "\t Train Loss 0.049 | Train F1-score: 0.978\n",
      "\t Validation Loss 0.312 | Validation F1-score: 0.873\n",
      "Best Validation Loss: 0.236\n",
      "Validation Accuracy at Best Loss: 0.918\n",
      "Validation F1-score: 0.870\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=64, dropout=0.3, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.684 | Train F1-score: 0.057\n",
      "\t Validation Loss 0.659 | Validation F1-score: 0.0\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.567 | Train F1-score: 0.417\n",
      "\t Validation Loss 0.448 | Validation F1-score: 0.676\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.354 | Train F1-score: 0.776\n",
      "\t Validation Loss 0.359 | Validation F1-score: 0.756\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.272 | Train F1-score: 0.84\n",
      "\t Validation Loss 0.317 | Validation F1-score: 0.802\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.212 | Train F1-score: 0.879\n",
      "\t Validation Loss 0.316 | Validation F1-score: 0.783\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.167 | Train F1-score: 0.91\n",
      "\t Validation Loss 0.298 | Validation F1-score: 0.806\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.134 | Train F1-score: 0.931\n",
      "\t Validation Loss 0.266 | Validation F1-score: 0.843\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 24s\n",
      "\t Train Loss 0.112 | Train F1-score: 0.947\n",
      "\t Validation Loss 0.254 | Validation F1-score: 0.863\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.092 | Train F1-score: 0.959\n",
      "\t Validation Loss 0.256 | Validation F1-score: 0.865\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 24s\n",
      "\t Train Loss 0.082 | Train F1-score: 0.961\n",
      "\t Validation Loss 0.254 | Validation F1-score: 0.855\n",
      "Best Validation Loss: 0.254\n",
      "Validation Accuracy at Best Loss: 0.906\n",
      "Validation F1-score: 0.855\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=32, dropout=0.5, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 35s\n",
      "\t Train Loss 0.364 | Train F1-score: 0.723\n",
      "\t Validation Loss 0.247 | Validation F1-score: 0.835\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.167 | Train F1-score: 0.9\n",
      "\t Validation Loss 0.248 | Validation F1-score: 0.847\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.095 | Train F1-score: 0.951\n",
      "\t Validation Loss 0.263 | Validation F1-score: 0.834\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.065 | Train F1-score: 0.964\n",
      "\t Validation Loss 0.334 | Validation F1-score: 0.812\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.051 | Train F1-score: 0.973\n",
      "\t Validation Loss 0.31 | Validation F1-score: 0.839\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.028 | Train F1-score: 0.987\n",
      "\t Validation Loss 0.369 | Validation F1-score: 0.819\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.021 | Train F1-score: 0.99\n",
      "\t Validation Loss 0.401 | Validation F1-score: 0.826\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.017 | Train F1-score: 0.991\n",
      "\t Validation Loss 0.36 | Validation F1-score: 0.851\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.014 | Train F1-score: 0.991\n",
      "\t Validation Loss 0.529 | Validation F1-score: 0.832\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.016 | Train F1-score: 0.992\n",
      "\t Validation Loss 0.525 | Validation F1-score: 0.827\n",
      "Best Validation Loss: 0.247\n",
      "Validation Accuracy at Best Loss: 0.893\n",
      "Validation F1-score: 0.835\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=64, dropout=0.5, lr=0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.45 | Train F1-score: 0.633\n",
      "\t Validation Loss 0.288 | Validation F1-score: 0.808\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.205 | Train F1-score: 0.874\n",
      "\t Validation Loss 0.274 | Validation F1-score: 0.822\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 24s\n",
      "\t Train Loss 0.121 | Train F1-score: 0.93\n",
      "\t Validation Loss 0.266 | Validation F1-score: 0.842\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.075 | Train F1-score: 0.958\n",
      "\t Validation Loss 0.279 | Validation F1-score: 0.859\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.056 | Train F1-score: 0.97\n",
      "\t Validation Loss 0.274 | Validation F1-score: 0.83\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.035 | Train F1-score: 0.983\n",
      "\t Validation Loss 0.369 | Validation F1-score: 0.839\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.027 | Train F1-score: 0.987\n",
      "\t Validation Loss 0.377 | Validation F1-score: 0.832\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.017 | Train F1-score: 0.991\n",
      "\t Validation Loss 0.479 | Validation F1-score: 0.839\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.021 | Train F1-score: 0.988\n",
      "\t Validation Loss 0.449 | Validation F1-score: 0.819\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.018 | Train F1-score: 0.99\n",
      "\t Validation Loss 0.427 | Validation F1-score: 0.82\n",
      "Best Validation Loss: 0.266\n",
      "Validation Accuracy at Best Loss: 0.894\n",
      "Validation F1-score: 0.842\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=32, dropout=0.5, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.661 | Train F1-score: 0.094\n",
      "\t Validation Loss 0.562 | Validation F1-score: 0.448\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.406 | Train F1-score: 0.708\n",
      "\t Validation Loss 0.346 | Validation F1-score: 0.743\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.286 | Train F1-score: 0.818\n",
      "\t Validation Loss 0.3 | Validation F1-score: 0.806\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.224 | Train F1-score: 0.871\n",
      "\t Validation Loss 0.288 | Validation F1-score: 0.821\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.175 | Train F1-score: 0.906\n",
      "\t Validation Loss 0.267 | Validation F1-score: 0.843\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 28s\n",
      "\t Train Loss 0.15 | Train F1-score: 0.917\n",
      "\t Validation Loss 0.254 | Validation F1-score: 0.836\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.124 | Train F1-score: 0.935\n",
      "\t Validation Loss 0.254 | Validation F1-score: 0.858\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 29s\n",
      "\t Train Loss 0.11 | Train F1-score: 0.944\n",
      "\t Validation Loss 0.245 | Validation F1-score: 0.856\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 27s\n",
      "\t Train Loss 0.09 | Train F1-score: 0.956\n",
      "\t Validation Loss 0.253 | Validation F1-score: 0.861\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 32s\n",
      "\t Train Loss 0.077 | Train F1-score: 0.959\n",
      "\t Validation Loss 0.257 | Validation F1-score: 0.862\n",
      "Best Validation Loss: 0.245\n",
      "Validation Accuracy at Best Loss: 0.909\n",
      "Validation F1-score: 0.856\n",
      "Menguji kombinasi: hidden_dim=128, batch_size=64, dropout=0.5, lr=0.0001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.681 | Train F1-score: 0.023\n",
      "\t Validation Loss 0.663 | Validation F1-score: 0.0\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.616 | Train F1-score: 0.174\n",
      "\t Validation Loss 0.504 | Validation F1-score: 0.569\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 25s\n",
      "\t Train Loss 0.399 | Train F1-score: 0.723\n",
      "\t Validation Loss 0.37 | Validation F1-score: 0.735\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 24s\n",
      "\t Train Loss 0.307 | Train F1-score: 0.806\n",
      "\t Validation Loss 0.325 | Validation F1-score: 0.796\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.25 | Train F1-score: 0.854\n",
      "\t Validation Loss 0.321 | Validation F1-score: 0.759\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.21 | Train F1-score: 0.883\n",
      "\t Validation Loss 0.286 | Validation F1-score: 0.826\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.172 | Train F1-score: 0.903\n",
      "\t Validation Loss 0.28 | Validation F1-score: 0.837\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.145 | Train F1-score: 0.924\n",
      "\t Validation Loss 0.261 | Validation F1-score: 0.816\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 22s\n",
      "\t Train Loss 0.133 | Train F1-score: 0.935\n",
      "\t Validation Loss 0.247 | Validation F1-score: 0.853\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 23s\n",
      "\t Train Loss 0.114 | Train F1-score: 0.941\n",
      "\t Validation Loss 0.25 | Validation F1-score: 0.843\n",
      "Best Validation Loss: 0.247\n",
      "Validation Accuracy at Best Loss: 0.906\n",
      "Validation F1-score: 0.853\n",
      "Best Hyperparameters: {'hidden_dim': 128, 'dropout': 0.3, 'n_layers': 2, 'learning_rate': 0.0001, 'batch_size': 32}\n",
      "Best Validation F1-score: 0.873\n",
      "Grafik kombinasi terbaik disimpan di folder training_plots.\n"
     ]
    }
   ],
   "source": [
    "# Jumlah total kata dalam kosakata (vocabulary) sebagai ukuran input\n",
    "INPUT_DIM = len(TEXT.vocab)  \n",
    "\n",
    "# Dimensi embedding harus sama dengan dimensi vektor fastText yang telah dilatih sebelumnya\n",
    "EMBEDDING_DIM = 300 \n",
    "OUTPUT_DIM = 1     \n",
    "\n",
    "# Menggunakan Bidirectional LSTM\n",
    "BIDIRECTIONAL = True    \n",
    "\n",
    "# Mendapatkan indeks token padding dari kosakata\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# Jumlah lapisan BiLSTM\n",
    "N_LAYERS = [2] # [1, 2] (untuk layer karena hanya satu nilai, pada perintah print dan lainnya bisa ditidakan)\n",
    "\n",
    "# Jumlah unit BiLSTM/Ukuran hidden state di dalam layer BiLSTM\n",
    "HIDDEN_DIM = [64, 128] # [32, 64, 128]\n",
    "\n",
    "# Probabilitas dropout untuk regularisasi, menghindari overfitting\n",
    "DROPOUT = [0.3, 0.5] # [0.1, 0.3, 0.5]\n",
    "\n",
    "# Ukuran batch\n",
    "BATCH_SIZE = [32, 64] # [16, 32, 64]\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = [1e-3, 1e-4] # [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Variabel untuk menyimpan semua hasil kombinasi hyperparameter\n",
    "all_results = []\n",
    "\n",
    "# Variabel untuk menyimpan hasil terbaik\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "\n",
    "# Membuat folder untuk model terbaik dari setiap kombinasi hyperparameter\n",
    "os.makedirs(\"best_models\", exist_ok=True)\n",
    "\n",
    "# Iterasi semua kombinasi hyperparameter\n",
    "for hidden_dim, dropout, n_layer, lr, batch_size in product(HIDDEN_DIM, DROPOUT, N_LAYERS, LEARNING_RATE, BATCH_SIZE):\n",
    "    print(f\"Menguji kombinasi: hidden_dim={hidden_dim}, batch_size={batch_size}, dropout={dropout}, lr={lr}\")\n",
    "    \n",
    "    # Membuat instance dari kelas LSTM dengan parameter yang sudah didefinisikan sebelumnya\n",
    "    model = LSTM(INPUT_DIM,\n",
    "            EMBEDDING_DIM,\n",
    "            hidden_dim,  # Gunakan nilai dari loop\n",
    "            OUTPUT_DIM,\n",
    "            n_layer,  # Gunakan nilai dari loop\n",
    "            BIDIRECTIONAL,\n",
    "            dropout,  # Gunakan nilai dari loop\n",
    "            PAD_IDX)\n",
    "                \n",
    "    # Tentukan ukuran batch\n",
    "    batch_size = batch_size  # Gunakan nilai dari loop\n",
    "\n",
    "    # Gunakan BucketIterator untuk membuat batch dari data training, validasi, dan testing\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),        \n",
    "        batch_size = batch_size,\n",
    "        device = torch.device('cpu'),             \n",
    "        sort_key = lambda x: len(x.processed_text), \n",
    "        sort_within_batch = True                    \n",
    "    )\n",
    "\n",
    "    # Menyalin embedding kata yang sudah dilatih sebelumnya (misalnya, fastText) ke dalam layer embedding model\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "    # Mengganti bobot awal layer embedding dengan embedding yang sudah dilatih sebelumnya\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    # Inisialisasi token <unk> (kata yang tidak dikenal) dan <pad> menjadi nol\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]  \n",
    "\n",
    "    # Mengatur baris dalam matriks embedding untuk <unk> dan <pad> menjadi nol\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    # Pastikan kosakata tetap sama di setiap iterasi loop hyperparameter\n",
    "    vocab_tokens = list(TEXT.vocab.stoi.keys())\n",
    "\n",
    "    # Konversi matriks embedding ke dalam DataFrame untuk disimpan ke CSV\n",
    "    embedding_df = pd.DataFrame(\n",
    "        model.embedding.weight.data.cpu().numpy(),\n",
    "        columns=[f\"fitur_{i+1}\" for i in range(EMBEDDING_DIM)]\n",
    "    )\n",
    "\n",
    "    # Gunakan hanya token yang sesuai dengan panjang embedding_df\n",
    "    embedding_df.insert(0, 'token', vocab_tokens[:len(embedding_df)])\n",
    "\n",
    "    # Menyimpan hasil ke dalam file CSV\n",
    "    embedding_df.to_csv('embedding_matrix.csv', index=False)\n",
    "\n",
    "    # Optimizer \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Menggunakan CPU untuk pelatihan\n",
    "    model = model.to(torch.device(\"cpu\"))\n",
    "    criterion = criterion.to(torch.device(\"cpu\"))\n",
    "\n",
    "    # Fungsi pembantu (helper functions)\n",
    "    def batch_accuracy(predictions, label):\n",
    "        preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (preds == label).float()\n",
    "        accuracy = correct.sum() / len(correct)\n",
    "        return accuracy\n",
    "\n",
    "    def timer(start_time, end_time):\n",
    "        time = end_time - start_time\n",
    "        mins = int(time / 60)\n",
    "        secs = int(time - (mins * 60))\n",
    "        return mins, secs\n",
    "\n",
    "    def train(model, iterator, optimizer, criterion):\n",
    "        training_loss = 0.0\n",
    "        training_acc = 0.0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        model.train()\n",
    "        for batch in iterator:\n",
    "\n",
    "            # 1. Mengatur gradien menjadi nol sebelum backward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # batch.text adalah tuple (tensor, panjang dari sequence)\n",
    "            text, text_lengths = batch.processed_text\n",
    "            \n",
    "            # 2. Menghitung prediksi dari model\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            # 3. Menghitung loss\n",
    "            loss = criterion(predictions, batch.vader_sentiment)\n",
    "            \n",
    "            # Menghitung akurasi per batch\n",
    "            accuracy = batch_accuracy(predictions, batch.vader_sentiment)\n",
    "            \n",
    "            # 4. Menggunakan loss untuk menghitung gradien\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. Menggunakan optimizer untuk memperbarui parameter model\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Menambahkan nilai loss dan akurasi batch ke total akumulatif\n",
    "            training_loss += loss.item()\n",
    "            training_acc += accuracy.item()\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(predictions)).detach().cpu().numpy()\n",
    "            labels = batch.vader_sentiment.cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(labels)\n",
    "\n",
    "        # Menghitung F1-score setelah semua batch diproses\n",
    "        f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "        # Mengembalikan nilai loss dan akurasi rata-rata untuk setiap epoch\n",
    "        return training_loss / len(iterator), training_acc / len(iterator), f1\n",
    "\n",
    "    def evaluate(model, iterator, criterion):\n",
    "        eval_loss = 0.0 # Menyimpan total loss selama evaluasi\n",
    "        eval_acc = 0.0 # Menyimpan total akurasi selama evaluasi\n",
    "        y_true = []  # Menyimpan label asli\n",
    "        y_pred = []  # Menyimpan prediksi model\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                text, text_lengths = batch.processed_text\n",
    "                predictions = model(text, text_lengths).squeeze(1)\n",
    "                loss = criterion(predictions, batch.vader_sentiment)\n",
    "                accuracy = batch_accuracy(predictions, batch.vader_sentiment)\n",
    "                eval_loss += loss.item()\n",
    "                eval_acc += accuracy.item()\n",
    "\n",
    "                preds = torch.round(torch.sigmoid(predictions)).cpu().numpy()\n",
    "                labels = batch.vader_sentiment.cpu().numpy()\n",
    "                y_pred.extend(preds)\n",
    "                y_true.extend(labels)\n",
    "        \n",
    "        # Menghitung F1-score setelah semua batch diproses\n",
    "        f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "        # Mengembalikan nilai loss dan akurasi rata-rata\n",
    "        return eval_loss / len(iterator), eval_acc / len(iterator), f1\n",
    "    \n",
    "    # Jumlah epoch untuk melatih model\n",
    "    NUM_EPOCHS = 10\n",
    "\n",
    "    # Inisialisasi nilai terendah untuk validasi loss\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    # List untuk menyimpan nilai loss dan accuracy pada setiap epoch\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accuracies, valid_accuracies = [], []\n",
    "\n",
    "    # Membuka file CSV untuk menyimpan hasil pelatihan\n",
    "    with open('training_logs.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        fieldnames = ['Epoch', 'Train Loss', 'Train Accuracy (%)', 'Validation Loss', 'Validation Accuracy (%)', 'Validation F1-score', 'Time (mins)', 'Time (secs)']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "    # Loop untuk setiap epoch\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time() \n",
    "        \n",
    "        # Menghitung loss dan akurasi pada data latihan\n",
    "        train_loss, train_acc, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "        \n",
    "        # Menghitung loss dan akurasi pada data validasi\n",
    "        valid_loss, valid_acc, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        mins, secs = timer(start_time, end_time)\n",
    "\n",
    "        # Simpan hasil pelatihan ke dalam list\n",
    "        all_results.append({\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'dropout': dropout,\n",
    "            'n_layers': n_layer,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': round(train_loss, 3),\n",
    "            'train_acc': round(train_acc * 100, 3),\n",
    "            'train_f1': round(train_f1, 3),\n",
    "            'valid_loss': round(valid_loss, 3),\n",
    "            'valid_acc': round(valid_acc * 100, 3),\n",
    "            'valid_f1': round(valid_f1, 3),\n",
    "            'time_mins': mins,\n",
    "            'time_secs': secs\n",
    "        })\n",
    "        \n",
    "        # Mengecek apakah validasi loss saat ini lebih baik dari sebelumnya dan menyimpan model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            best_valid_f1 = valid_f1\n",
    "            model_filename = f\"model_h{hidden_dim}_bs{batch_size}_d{dropout}_lr{lr}.pt\"\n",
    "            model_path = os.path.join(\"best_models\", model_filename)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Menampilkan hasil dari setiap epoch\n",
    "        print(\"Epoch {}:\".format(epoch+1))\n",
    "        print(\"\\t Total Time: {}m {}s\".format(mins, secs))\n",
    "        print(\"\\t Train Loss {} | Train F1-score: {}\".format(round(train_loss, 3), round(train_f1, 3)))\n",
    "        print(\"\\t Validation Loss {} | Validation F1-score: {}\".format(round(valid_loss, 3), round(valid_f1, 3)))\n",
    "\n",
    "    # Cek apakah kombinasi ini lebih baik dari sebelumnya\n",
    "    if valid_f1 > best_f1:\n",
    "        best_f1 = valid_f1 \n",
    "        best_acc = valid_acc \n",
    "        best_params = {\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"dropout\": dropout,\n",
    "            \"n_layers\": n_layer,\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": batch_size\n",
    "        }\n",
    "\n",
    "    print(f\"Best Validation Loss: {best_valid_loss:.3f}\")\n",
    "    print(f\"Validation Accuracy at Best Loss: {best_valid_acc:.3f}\")\n",
    "    print(f\"Validation F1-score: {best_valid_f1:.3f}\")\n",
    "\n",
    "# Simpan semua hasil ke dalam file CSV\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(\"training_logs.csv\", index=False)\n",
    "\n",
    "# Ambil epoch terbaik (berdasarkan valid_loss) untuk setiap kombinasi unik\n",
    "best_epoch_per_config = (\n",
    "    df_results.sort_values(\"valid_loss\")  # Urutkan dari valid_loss terkecil\n",
    "              .drop_duplicates(subset=[\"hidden_dim\", \"dropout\", \"n_layers\", \"learning_rate\", \"batch_size\"], keep=\"first\")\n",
    "              .loc[:, [\"hidden_dim\", \"dropout\", \"n_layers\", \"learning_rate\", \"batch_size\",\n",
    "                       \"epoch\", \"train_loss\", \"valid_loss\", \"train_f1\", \"valid_f1\"]]\n",
    ")\n",
    "\n",
    "# Simpan ke file CSV\n",
    "best_epoch_per_config.to_csv(\"best_epoch_per_config.csv\", index=False)\n",
    "\n",
    "# Semua baris kode dibawah ini dipindahkan ke sel lain?\n",
    "# Tampilkan konfigurasi hyperparameter terbaik\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Best Validation F1-score: {round(best_f1, 3)}\")\n",
    "\n",
    "# Buat folder untuk menyimpan grafik jika belum ada\n",
    "os.makedirs(\"training_plots\", exist_ok=True)\n",
    "\n",
    "# Ambil hyperparameter terbaik\n",
    "best_h_dim = best_params[\"hidden_dim\"]\n",
    "best_dropout = best_params[\"dropout\"]\n",
    "best_n_layer = best_params[\"n_layers\"]\n",
    "best_lr = best_params[\"learning_rate\"]\n",
    "best_batch_size = best_params[\"batch_size\"]\n",
    "\n",
    "# Filter hasil untuk kombinasi terbaik\n",
    "best_results = [r for r in all_results if r[\"hidden_dim\"] == best_h_dim and \n",
    "                                            r[\"dropout\"] == best_dropout and \n",
    "                                            r[\"n_layers\"] == best_n_layer and \n",
    "                                            r[\"learning_rate\"] == best_lr and \n",
    "                                            r[\"batch_size\"] == best_batch_size]\n",
    "\n",
    "# Ambil nilai loss dan akurasi dari hasil terbaik\n",
    "epochs = [r[\"epoch\"] for r in best_results]\n",
    "train_losses = [r[\"train_loss\"] for r in best_results]\n",
    "valid_losses = [r[\"valid_loss\"] for r in best_results]\n",
    "train_accs = [r[\"train_acc\"] for r in best_results]\n",
    "valid_accs = [r[\"valid_acc\"] for r in best_results]\n",
    "train_f1_scores = [r[\"train_f1\"] for r in best_results]\n",
    "valid_f1_scores = [r[\"valid_f1\"] for r in best_results]\n",
    "\n",
    "# Buat grafik loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(epochs, valid_losses, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Loss Plot | Best: h={best_h_dim}, b={best_batch_size}, d={best_dropout}, lr={best_lr}\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"training_plots/best_loss_h{best_h_dim}_b{best_batch_size}_d{best_dropout}_lr{best_lr}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Buat grafik F1-score\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_f1_scores, label=\"Train F1-score\", marker='o')\n",
    "plt.plot(epochs, valid_f1_scores, label=\"Validation F1-score\", marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(f\"F1-score Plot | Best: h={best_h_dim}, b={best_batch_size}, d={best_dropout}, lr={best_lr}\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"training_plots/best_f1_h{best_h_dim}_b{best_batch_size}_d{best_dropout}_lr{best_lr}.png\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Grafik kombinasi terbaik disimpan di folder training_plots.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretasi Setiap Bagian Output*\n",
    "\n",
    "    Epoch:\n",
    "    Setiap epoch merepresentasikan satu siklus penuh seluruh data latih yang diproses oleh model. Di sini, ada n epoch yang berarti model dilatih sebanyak n kali dengan seluruh dataset latih.\n",
    "\n",
    "    Total Time:\n",
    "    Menunjukkan total waktu yang diperlukan untuk menyelesaikan satu epoch. Misalnya, epoch 1 memakan waktu 1 menit.\n",
    "\n",
    "    Train Loss dan Validation Loss:\n",
    "    Train Loss: Merupakan nilai loss atau kerugian yang dihasilkan model pada data latih di setiap epoch. Nilai ini dihitung berdasarkan perbedaan antara prediksi model dengan label sebenarnya.\n",
    "    Validation Loss: Merupakan nilai loss atau kerugian yang dihasilkan model pada data validasi. Ini dihitung dengan cara yang sama seperti train loss, tetapi pada data yang tidak digunakan untuk melatih model.\n",
    "    Interpretasi:\n",
    "    Penurunan Train Loss:\n",
    "    Jika train loss menurun seiring epoch bertambah, model berhasil mempelajari pola-pola pada data latih.\n",
    "    Penurunan Validation Loss:\n",
    "    Penurunan pada validation loss menunjukkan bahwa model mempelajari pola yang juga berlaku pada data yang belum dilihat sebelumnya. Namun, jika validation loss justru meningkat setelah beberapa epoch, hal ini bisa menjadi indikasi bahwa model mulai overfitting, yaitu terlalu berfokus pada data latih dan kehilangan kemampuan untuk generalisasi pada data baru.\n",
    "\n",
    "    Train Accuracy dan Validation Accuracy:\n",
    "    Train Accuracy:\n",
    "    Akurasi model pada data latih, menunjukkan persentase prediksi model yang benar pada data latih.\n",
    "    Validation Accuracy:\n",
    "    Akurasi model pada data validasi, menunjukkan persentase prediksi model yang benar pada data validasi.\n",
    "    Interpretasi:\n",
    "    Akurasi pada Data Latih:\n",
    "    Akurasi yang meningkat menunjukkan bahwa model semakin baik dalam memprediksi data latih.\n",
    "    Akurasi pada Data Validasi:\n",
    "    Akurasi ini lebih penting karena menunjukkan seberapa baik model memprediksi data yang belum dilihat sebelumnya. Jika akurasi validasi menurun setelah beberapa epoch meskipun akurasi latih meningkat, ini mengindikasikan overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRHo-gVszo9H"
   },
   "source": [
    "<a id='section7'></a>\n",
    "# **7. Klasifikasi Sentimen**\n",
    "Pada tahap klasifikasi sentimen, model BiLSTM yang telah dilatih sebelumnya kemudian akan diuji pada data uji. Data ini tidak terlibat sama sekali dalam proses pelatihan, jadi data ini baru bagi model dan berfungsi menguji performa penerapan model pada data ulasan. Model dengan f1-score tertinggi pada tahap pelatihan yang akan digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "n40NoCEniiL6",
    "outputId": "5bfdc497-c2de-4f0c-d3fe-b722422fd4ec"
   },
   "outputs": [],
   "source": [
    "# Memanggil model dengan f1-score terbaik\n",
    "model.load_state_dict(torch.load(f\"best_models/model_h{best_params['hidden_dim']}_bs{best_params['batch_size']}_d{best_params['dropout']}_lr{best_params['learning_rate']}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEzb31Turmm_"
   },
   "outputs": [],
   "source": [
    "def predict(model, text, tokenized=True):\n",
    "   \n",
    "    # Menentukan perangkat yang akan digunakan, apakah GPU atau CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Mengatur model ke mode evaluasi (tanpa menghitung gradien)\n",
    "    model.eval()\n",
    "\n",
    "    if tokenized == False:\n",
    "        tokens = [token.text for token in nlp.tokenizer(text)]\n",
    "    else:\n",
    "        tokens = text\n",
    "\n",
    "    indexed_tokens = [TEXT.vocab.stoi[t] for t in tokens]\n",
    "    length = [len(indexed_tokens)]\n",
    "    tensor = torch.LongTensor(indexed_tokens).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "\n",
    "    # Mengembalikan hasil prediksi sebagai nilai\n",
    "    return 1 - prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "LNVfJSHooMus",
    "outputId": "9a7ccf8a-a352-4bf9-a025-1425d1e3761d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ulasan: prngemudi ga bener chat telfon semua ga respon burukkkkk\n",
      "Prediksi Sentimen: 0.0\n",
      "Label Asli: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Contoh prediksi sentimen dari satu ulasan dalam test set\n",
    "print(\"Ulasan: {}\".format(TreebankWordDetokenizer().detokenize(test_data[1].processed_text)))\n",
    "print(\"Prediksi Sentimen: {}\".format(round(predict(model, test_data[1].processed_text), 0)))\n",
    "print(\"Label Asli: {}\".format(test_data[1].vader_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "8GD0zC4Fugk-",
    "outputId": "6f14da85-f61e-401e-fc07-d33533ec7250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi sentimen telah disimpan ke 'hasil_prediksi.csv'\n",
      "                                              Ulasan  Prediksi Sentimen  \\\n",
      "0                                 baik bgt drivernya           0.998550   \n",
      "1  prngemudi ga bener chat telfon semua ga respon...           0.022159   \n",
      "2                          bagus sekali kemudi ramah           0.999974   \n",
      "3  aplikasi gajelas cari map aja susah benerin bi...           0.273140   \n",
      "4                                               baik           0.995369   \n",
      "\n",
      "   Label Prediksi  Label Asli  \n",
      "0             1.0         1.0  \n",
      "1             0.0         0.0  \n",
      "2             1.0         1.0  \n",
      "3             0.0         1.0  \n",
      "4             1.0         1.0  \n"
     ]
    }
   ],
   "source": [
    "# Contoh prediksi sentimen dari seluruh ulasan dalam test set\n",
    "# List untuk menyimpan data prediksi\n",
    "d = []\n",
    "\n",
    "# Loop melalui seluruh ulasan dalam test set\n",
    "for idx in range(len(test_data)):\n",
    "\n",
    "    # Detokenisasi ulasan dari test set\n",
    "    ulasan = TreebankWordDetokenizer().detokenize(test_data[idx].processed_text)\n",
    "    \n",
    "    # Prediksi sentimen dari ulasan\n",
    "    prediksi = predict(model, test_data[idx].processed_text)\n",
    "    \n",
    "    # Membulatkan nilai prediksi menjadi label (0 atau 1)\n",
    "    label_prediksi = round(prediksi, 0)\n",
    "    \n",
    "    # Menyimpan hasil ulasan, prediksi, label prediksi, dan label asli ke dalam list\n",
    "    d.append({\n",
    "        'Ulasan': ulasan, \n",
    "        'Prediksi Sentimen': prediksi, \n",
    "        'Label Prediksi': label_prediksi,\n",
    "        'Label Asli': test_data[idx].vader_sentiment\n",
    "    })\n",
    "\n",
    "# Konversi list ke dataframe untuk tampilan yang lebih baik\n",
    "df_prediksi = pd.DataFrame(d)\n",
    "\n",
    "# Simpan hasil prediksi ke file CSV\n",
    "df_prediksi.to_csv('hasil_prediksi.csv', index=False)\n",
    "\n",
    "print(\"Prediksi sentimen telah disimpan ke 'hasil_prediksi.csv'\")\n",
    "\n",
    "# Tampilkan beberapa hasil sebagai contoh\n",
    "print(df_prediksi.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "# **8. Evaluasi Model**\n",
    "Pada tahap evaluasi model, performa model BiLSTM pada data uji akan dievaluasi berdasarkan beberapa metrik, yaitu accuracy, precision, dan recall. Hasil prediksi juga akan disajikan dalam bentuk confusion matrix, dimana bisa dilihat secara jelas jumlah klasifikasi yang sesuai atau tidak sesuai label aslinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi evaluasi model untuk klasifikasi dua kelas (positif dan negatif)\n",
    "def evaluate_model(model, iterator, criterion):\n",
    "                          \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.processed_text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "            all_predictions.extend(rounded_preds.cpu().numpy())\n",
    "            all_labels.extend(batch.vader_sentiment.cpu().numpy())\n",
    "\n",
    "    # Hitung metrik evaluasi untuk binary classification\n",
    "    report = classification_report(all_labels, all_predictions, target_names=['Positif', 'Negatif'], output_dict=True)\n",
    "    \n",
    "    # Ambil metrik untuk sentimen positif\n",
    "    accuracy = report['accuracy']\n",
    "    positive_precision = report['Positif']['precision']\n",
    "    positive_recall = report['Positif']['recall']\n",
    "    positive_f1 = report['Positif']['f1-score']\n",
    "    \n",
    "    # Tampilkan classification report lengkap\n",
    "    print(classification_report(all_labels, all_predictions, target_names=['Positif', 'Negatif']))\n",
    "\n",
    "    # Buat confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Visualisasi confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Positif', 'Negatif'], yticklabels=['Positif', 'Negatif'])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, positive_precision, positive_recall, positive_f1, all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Positif       0.96      0.95      0.95       533\n",
      "     Negatif       0.90      0.91      0.91       252\n",
      "\n",
      "    accuracy                           0.94       785\n",
      "   macro avg       0.93      0.93      0.93       785\n",
      "weighted avg       0.94      0.94      0.94       785\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "Precision: 0.96\n",
      "Recall: 0.95\n",
      "F1 Score: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Hitung metrik evaluasi pada data test\n",
    "accuracy, precision, recall, f1, all_labels, all_predictions = evaluate_model(model, test_iterator, criterion)\n",
    "\n",
    "# Tampilkan hasil metrik untuk sentimen positif\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kode Bersih**\n",
    "Kode yang lebih bersih dan padat guna mengefisienkan ruang. Digunakan untuk kebutuhan skripsi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kode per langkah preprocessing\n",
    "# Step 1: Cleansing text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Step 2: Case folding\n",
    "def case_folding(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Step 3: Tokenisasi\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc] \n",
    "\n",
    "# Step 4: Menghapus stopword menggunakan daftar dari Sastrawi\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_list = set(stopword_factory.get_stop_words()) \n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopword_list]\n",
    "\n",
    "# Step 5: Stemming menggunakan Sastrawi\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "def stemming(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menyimpan hasil setiap langkah ke CSV\n",
    "def save_to_csv(df, step_name):\n",
    "    filename = f'processed_data_{step_name}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {step_name} result to {filename}\")\n",
    "\n",
    "# Fungsi utama untuk memproses dan menyimpan setiap langkah\n",
    "def process_and_save_steps(df):\n",
    "    # Step 1: Cleansing text\n",
    "    df['cleaned_text'] = df['content'].apply(clean_text)\n",
    "    save_to_csv(df[['content', 'cleaned_text']], 'cleansing')\n",
    "\n",
    "    # Step 2: Case folding\n",
    "    df['case_folding'] = df['cleaned_text'].apply(case_folding)\n",
    "    save_to_csv(df[['cleaned_text', 'case_folding']], 'case_folding')\n",
    "\n",
    "    # Step 3: Tokenisasi\n",
    "    df['tokenized_text'] = df['case_folding'].apply(tokenize)\n",
    "    save_to_csv(df[['case_folding', 'tokenized_text']], 'tokenization')\n",
    "\n",
    "    # Step 4: Penghapusan stopword\n",
    "    df['no_stopwords'] = df['tokenized_text'].apply(remove_stopwords)\n",
    "    save_to_csv(df[['tokenized_text', 'no_stopwords']], 'stopword_removal')\n",
    "\n",
    "    # Step 5: Stemming\n",
    "    df['stemmed_text'] = df['no_stopwords'].apply(stemming)\n",
    "    save_to_csv(df[['no_stopwords', 'stemmed_text']], 'stemming')\n",
    "\n",
    "# Membaca file CSV sebagai DataFrame\n",
    "df = pd.read_csv('review_content.csv')\n",
    "\n",
    "# Menjalankan proses preprocessing\n",
    "process_and_save_steps(df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment140",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "723d4b7bc280cd31fdada53ad6420192b9a3a8d60631096143cc718cb9440dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
